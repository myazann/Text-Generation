{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Zxvnx4z8gtfuAj4J-WzKEAXP4SIpq0Y1",
      "authorship_tag": "ABX9TyNeUBcStsVBKDwJpilEGe0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myazann/Text-Generation/blob/main/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h96-7s68ujrR"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install datasets==1.0.2\r\n",
        "\r\n",
        "!pip install git-python==1.0.3\r\n",
        "!pip install sacrebleu==1.4.12\r\n",
        "! pip install tokenizers\r\n",
        "\r\n",
        "import os\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from transformers import OpenAIGPTLMHeadModel, Trainer, TrainingArguments, GPT2LMHeadModel\r\n",
        "from transformers import AutoModel, AutoModelWithLMHead, AutoTokenizer, EncoderDecoderModel, BertTokenizer\r\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling,LineByLineTextDataset\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from transformers import pipeline\r\n",
        "import torch\r\n",
        "import gc\r\n",
        "import datasets\r\n",
        "from dataclasses import dataclass, field\r\n",
        "from typing import Optional\r\n",
        "from tokenizers import BertWordPieceTokenizer, Tokenizer\r\n",
        "from tokenizers.processors import BertProcessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDzUdGqlzfk_"
      },
      "source": [
        "A tokenizer can be trained from the scratch, but I am going to use a pretrained one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6l6U36iJysq",
        "outputId": "7c1b41a6-a2cd-4da6-dfcb-e5918c1b3a28"
      },
      "source": [
        "tokenizer = BertWordPieceTokenizer()\r\n",
        "\r\n",
        "tokenizer.train(files=\"/content/drive/My Drive/telegram_chatbot_train.csv\", vocab_size=32_000, min_frequency=2)\r\n",
        "tokenizer.save_model(\".\")\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/drive/My Drive/vocab.txt\", do_lower_case=True, \r\n",
        "                                          return_special_tokens_mask=True, model_max_len = 512, is_fast = True)\r\n",
        "tokenizer.bos_token = tokenizer.cls_token\r\n",
        "tokenizer.eos_token = tokenizer.sep_token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D5YFYAqzmqe"
      },
      "source": [
        "Loading data, tokenizer and the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK2CQpwI9j1R"
      },
      "source": [
        "if os.listdir(\"/content/drive/My Drive/enc2dec\"):\r\n",
        "  last_trained_path = \"/content/drive/My Drive/enc2dec\" + \"/\" + os.listdir(\"/content/drive/My Drive/enc2dec\")[0]\r\n",
        "else:\r\n",
        "  last_trained_path = None \r\n",
        "\r\n",
        "tg_data_train = datasets.load_dataset(\"csv\", data_files = \"/content/drive/My Drive/telegram_chatbot_train.csv\", split = \"train\")\r\n",
        "tg_data_val = datasets.load_dataset(\"csv\", data_files = \"/content/drive/My Drive/telegram_chatbot_val.csv\", split = \"train\")\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-turkish-cased', do_lower_case=True, return_special_tokens_mask=True)\r\n",
        "tokenizer.bos_token = tokenizer.cls_token\r\n",
        "tokenizer.eos_token = tokenizer.sep_token\r\n",
        "\r\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"dbmdz/bert-base-turkish-cased\", \"dbmdz/bert-base-turkish-cased\")\r\n",
        "\r\n",
        "if last_trained_path is not None:\r\n",
        "  model = model.from_pretrained(last_trained_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvM6i6mmYvTh"
      },
      "source": [
        "Free memory if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOChStZLEuWB"
      },
      "source": [
        "model = None\r\n",
        "trainer = None\r\n",
        "training_args = None\r\n",
        "\r\n",
        "del model\r\n",
        "del training_args\r\n",
        "del trainer\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4mNh2OfY7R9"
      },
      "source": [
        "Creating the data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCwYpn5aL0kZ"
      },
      "source": [
        "def process_data_to_model_inputs(batch):\r\n",
        "  inputs = tokenizer(batch[\"Written_Text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\r\n",
        "  outputs = tokenizer(batch[\"Answer_Text\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\r\n",
        "\r\n",
        "  batch[\"input_ids\"] = inputs.input_ids\r\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\r\n",
        "  batch[\"decoder_input_ids\"] = outputs.input_ids\r\n",
        "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\r\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\r\n",
        "\r\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\r\n",
        "\r\n",
        "  return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Du7h3BACUvj"
      },
      "source": [
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\r\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\r\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\r\n",
        "\r\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\r\n",
        "model.config.max_length = 150\r\n",
        "model.config.min_length = 5\r\n",
        "\r\n",
        "model.config.no_repeat_ngram_size = 3 \r\n",
        "model.config.early_stopping = True\r\n",
        "model.config.length_penalty = 2.0\r\n",
        "model.config.num_beams = 4\r\n",
        "\r\n",
        "\r\n",
        "batch_size=16\r\n",
        "encoder_max_length=128\r\n",
        "decoder_max_length=128\r\n",
        "\r\n",
        "\r\n",
        "tg_data_train = tg_data_train.map(\r\n",
        "    process_data_to_model_inputs, \r\n",
        "    batched=True, \r\n",
        "    batch_size=batch_size,\r\n",
        "    remove_columns=[\"Written_Text\", \"Answer_Text\"]\r\n",
        "    )\r\n",
        "\r\n",
        "tg_data_train.set_format(\r\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\r\n",
        ")\r\n",
        "\r\n",
        "tg_data_val = tg_data_val.map(\r\n",
        "    process_data_to_model_inputs, \r\n",
        "    batched=True, \r\n",
        "    batch_size=batch_size, \r\n",
        "    remove_columns=[\"Written_Text\", \"Answer_Text\"]\r\n",
        ")\r\n",
        "tg_data_val.set_format(\r\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf3ngc9hZTkX"
      },
      "source": [
        "Instantiate a Trainer and start training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAYWiPCrkOmF"
      },
      "source": [
        "training_args = TrainingArguments(\r\n",
        "    output_dir = \"/content/drive/My Drive/enc2dec\",\r\n",
        "    per_device_train_batch_size=batch_size,\r\n",
        "    per_device_eval_batch_size=64,\r\n",
        "    ##predict_with_generate=True,\r\n",
        "    ##evaluation_strategy = \"steps\",\r\n",
        "    do_train=True,\r\n",
        "    do_eval=True,\r\n",
        "    save_total_limit = 1,\r\n",
        "    \r\n",
        "    logging_steps=2500,  \r\n",
        "    save_steps=2500,  \r\n",
        "    eval_steps=2500,  \r\n",
        "    warmup_steps=2500,  \r\n",
        "    max_steps=100000, \r\n",
        "    overwrite_output_dir=True,\r\n",
        "    fp16=True\r\n",
        ")\r\n",
        "\r\n",
        "# instantiate trainer\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,\r\n",
        "    args=training_args,\r\n",
        "    train_dataset=tg_data_train,\r\n",
        "    eval_dataset=tg_data_val,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_ADsiaHCwfE"
      },
      "source": [
        "if last_trained_path is None:\r\n",
        "  trainer.train()\r\n",
        "else: \r\n",
        "  trainer.train(last_trained_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtPmJ3onbNGk"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2XplqCuhCOC"
      },
      "source": [
        "!rm -rf runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}